{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anushkapatwa/CSCI-167/blob/main/Notebooks/Chap07/7_2_Backpropagation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Notebook 7.2: Backpropagation**\n",
        "\n",
        "This notebook runs the backpropagation algorithm on a deep neural network as described in section 7.4 of the book.\n",
        "\n",
        "Work through the cells below, running each cell in turn. In various places you will see the words \"TODO\". Follow the instructions at these places and make predictions about what is going to happen or write code to complete the functions.\n",
        "\n",
        "Contact me at udlbookmail@gmail.com if you find any mistakes or have any suggestions."
      ],
      "metadata": {
        "id": "L6chybAVFJW2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "LdIDglk1FFcG"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First let's define a neural network.  We'll just choose the weights and biases randomly for now"
      ],
      "metadata": {
        "id": "nnUoI0m6GyjC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def forward(x, all_weights, all_biases):\n",
        "    h = x\n",
        "    # hidden layers\n",
        "    for W, b in zip(all_weights[:-1], all_biases[:-1]):\n",
        "        h = relu(W @ h + b)\n",
        "    # output layer (linear)\n",
        "    out = all_weights[-1] @ h + all_biases[-1]\n",
        "    return out\n"
      ],
      "metadata": {
        "id": "WVM4Tc_jGI0Q"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the Rectified Linear Unit (ReLU) function\n",
        "def ReLU(preactivation):\n",
        "  activation = preactivation.clip(0.0)\n",
        "  return activation"
      ],
      "metadata": {
        "id": "jZh-7bPXIDq4"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's run our random network.  The weight matrices $\\boldsymbol\\Omega_{0\\ldots K}$ are the entries of the list \"all_weights\" and the biases $\\boldsymbol\\beta_{0\\ldots K}$ are the entries of the list \"all_biases\"\n",
        "\n",
        "We know that we will need the preactivations $\\mathbf{f}_{0\\ldots K}$ and the activations $\\mathbf{h}_{1\\ldots K}$ for the forward pass of backpropagation, so we'll store and return these as well.\n"
      ],
      "metadata": {
        "id": "5irtyxnLJSGX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def compute_network_output(net_input, all_weights, all_biases):\n",
        "    # Retrieve number of layers (excluding output)\n",
        "    K = len(all_weights) - 1\n",
        "\n",
        "    # Pre-activations and activations\n",
        "    all_f = [None] * (K+1)\n",
        "    all_h = [None] * (K+1)\n",
        "\n",
        "    # Input is activation of layer 0\n",
        "    all_h[0] = net_input\n",
        "\n",
        "    # Forward pass through hidden layers\n",
        "    for layer in range(K):\n",
        "        # pre-activation\n",
        "        all_f[layer] = np.matmul(all_weights[layer], all_h[layer]) + all_biases[layer]\n",
        "        # activation\n",
        "        all_h[layer+1] = relu(all_f[layer])\n",
        "\n",
        "    # Output layer (linear, no activation)\n",
        "    all_f[K] = np.matmul(all_weights[-1], all_h[K]) + all_biases[-1]\n",
        "\n",
        "    # Final network output\n",
        "    net_output = all_f[K]\n",
        "\n",
        "    return net_output, all_f, all_h\n"
      ],
      "metadata": {
        "id": "LgquJUJvJPaN"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of hidden layers\n",
        "K = 5\n",
        "D = 6\n",
        "D_i = 1\n",
        "D_o = 1\n",
        "\n",
        "# Make empty lists\n",
        "all_weights = [None] * (K+1)\n",
        "all_biases = [None] * (K+1)\n",
        "\n",
        "# Input and output layers\n",
        "all_weights[0] = np.random.normal(size=(D, D_i))\n",
        "all_weights[-1] = np.random.normal(size=(D_o, D))\n",
        "all_biases[0] = np.random.normal(size=(D,1))\n",
        "all_biases[-1]= np.random.normal(size=(D_o,1))\n",
        "\n",
        "# Intermediate hidden layers\n",
        "for layer in range(1,K):\n",
        "    all_weights[layer] = np.random.normal(size=(D,D))\n",
        "    all_biases[layer] = np.random.normal(size=(D,1))\n"
      ],
      "metadata": {
        "id": "IN6w5m2ZOhnB"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's define a loss function.  We'll just use the least squares loss function. We'll also write a function to compute dloss_doutput"
      ],
      "metadata": {
        "id": "SxVTKp3IcoBF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def least_squares_loss(net_output, y):\n",
        "  return np.sum((net_output-y) * (net_output-y))\n",
        "\n",
        "def d_loss_d_output(net_output, y):\n",
        "    return 2*(net_output -y);"
      ],
      "metadata": {
        "id": "6XqWSYWJdhQR"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Input\n",
        "net_input = np.ones((1,1)) * 1.2\n",
        "\n",
        "# Forward pass through network\n",
        "net_output, all_f, all_h = compute_network_output(net_input, all_weights, all_biases)\n",
        "\n",
        "# Target\n",
        "y = np.ones((1,1)) * 20.0   # since D_o = 1\n",
        "\n",
        "# Compute loss\n",
        "loss = least_squares_loss(net_output, y)\n",
        "print(\"y = %3.3f  Loss = %3.3f\"%(y, loss))\n"
      ],
      "metadata": {
        "id": "njF2DUQmfttR",
        "outputId": "b8311ec7-8f91-47f9-d440-c9e352eb2b49",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "y = 20.000  Loss = 12608.351\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-4185488072.py:12: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  print(\"y = %3.3f  Loss = %3.3f\"%(y, loss))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's compute the derivatives of the network.  We already computed the forward pass.  Let's compute the backward pass."
      ],
      "metadata": {
        "id": "98WmyqFYWA-0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def backward_pass(all_weights, all_biases, all_f, all_h, y):\n",
        "    # Store gradients\n",
        "    all_dl_dweights = [None] * (K+1)\n",
        "    all_dl_dbiases  = [None] * (K+1)\n",
        "    all_dl_df       = [None] * (K+1)\n",
        "    all_dl_dh       = [None] * (K+1)\n",
        "\n",
        "    # Derivative wrt network output\n",
        "    all_dl_df[K] = np.array(d_loss_d_output(all_f[K], y))\n",
        "\n",
        "    # Backward loop\n",
        "    for layer in range(K, -1, -1):\n",
        "        # Eq. 7.22: bias gradient\n",
        "        all_dl_dbiases[layer] = np.array(all_dl_df[layer])   # copy\n",
        "\n",
        "        # Eq. 7.23: weight gradient\n",
        "        all_dl_dweights[layer] = np.matmul(all_dl_df[layer], all_h[layer].T)\n",
        "\n",
        "        # Eq. 7.25 (first part): derivative wrt activations\n",
        "        all_dl_dh[layer] = np.matmul(all_weights[layer].T, all_dl_df[layer])\n",
        "\n",
        "        if layer > 0:\n",
        "            # Eq. 7.25 (second part): backprop through ReLU\n",
        "            relu_grad = indicator_function(all_f[layer-1])   # 1 if >0, else 0\n",
        "            all_dl_df[layer-1] = all_dl_dh[layer] * relu_grad\n",
        "\n",
        "    return all_dl_dweights, all_dl_dbiases\n"
      ],
      "metadata": {
        "id": "LJng7WpRPLMz"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def indicator_function(x):\n",
        "    x_in = np.array(x, copy=True)   # make a copy to avoid modifying original\n",
        "    x_in[x_in > 0] = 1\n",
        "    x_in[x_in <= 0] = 0\n",
        "    return x_in\n",
        "all_dl_dweights, all_dl_dbiases = backward_pass(all_weights, all_biases, all_f, all_h, y)\n"
      ],
      "metadata": {
        "id": "9A9MHc4sQvbp"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.set_printoptions(precision=3)\n",
        "# Make space for derivatives computed by finite differences\n",
        "all_dl_dweights_fd = [None] * (K+1)\n",
        "all_dl_dbiases_fd = [None] * (K+1)\n",
        "\n",
        "# Let's test if we have the derivatives right using finite differences\n",
        "delta_fd = 0.000001\n",
        "\n",
        "# Test the dervatives of the bias vectors\n",
        "for layer in range(K+1):\n",
        "  dl_dbias  = np.zeros_like(all_dl_dbiases[layer])\n",
        "  # For every element in the bias\n",
        "  for row in range(all_biases[layer].shape[0]):\n",
        "    # Take copy of biases  We'll change one element each time\n",
        "    all_biases_copy = [np.array(x) for x in all_biases]\n",
        "    all_biases_copy[layer][row] += delta_fd\n",
        "    network_output_1, *_ = compute_network_output(net_input, all_weights, all_biases_copy)\n",
        "    network_output_2, *_ = compute_network_output(net_input, all_weights, all_biases)\n",
        "    dl_dbias[row] = (least_squares_loss(network_output_1, y) - least_squares_loss(network_output_2,y))/delta_fd\n",
        "  all_dl_dbiases_fd[layer] = np.array(dl_dbias)\n",
        "  print(\"-----------------------------------------------\")\n",
        "  print(\"Bias %d, derivatives from backprop:\"%(layer))\n",
        "  print(all_dl_dbiases[layer])\n",
        "  print(\"Bias %d, derivatives from finite differences\"%(layer))\n",
        "  print(all_dl_dbiases_fd[layer])\n",
        "  if np.allclose(all_dl_dbiases_fd[layer],all_dl_dbiases[layer],rtol=1e-05, atol=1e-08, equal_nan=False):\n",
        "    print(\"Success!  Derivatives match.\")\n",
        "  else:\n",
        "    print(\"Failure!  Derivatives different.\")\n",
        "\n",
        "\n",
        "\n",
        "# Test the derivatives of the weights matrices\n",
        "for layer in range(K+1):\n",
        "  dl_dweight  = np.zeros_like(all_dl_dweights[layer])\n",
        "  # For every element in the bias\n",
        "  for row in range(all_weights[layer].shape[0]):\n",
        "    for col in range(all_weights[layer].shape[1]):\n",
        "      # Take copy of biases  We'll change one element each time\n",
        "      all_weights_copy = [np.array(x) for x in all_weights]\n",
        "      all_weights_copy[layer][row][col] += delta_fd\n",
        "      network_output_1, *_ = compute_network_output(net_input, all_weights_copy, all_biases)\n",
        "      network_output_2, *_ = compute_network_output(net_input, all_weights, all_biases)\n",
        "      dl_dweight[row][col] = (least_squares_loss(network_output_1, y) - least_squares_loss(network_output_2,y))/delta_fd\n",
        "  all_dl_dweights_fd[layer] = np.array(dl_dweight)\n",
        "  print(\"-----------------------------------------------\")\n",
        "  print(\"Weight %d, derivatives from backprop:\"%(layer))\n",
        "  print(all_dl_dweights[layer])\n",
        "  print(\"Weight %d, derivatives from finite differences\"%(layer))\n",
        "  print(all_dl_dweights_fd[layer])\n",
        "  if np.allclose(all_dl_dweights_fd[layer],all_dl_dweights[layer],rtol=1e-05, atol=1e-08, equal_nan=False):\n",
        "    print(\"Success!  Derivatives match.\")\n",
        "  else:\n",
        "    print(\"Failure!  Derivatives different.\")"
      ],
      "metadata": {
        "id": "PK-UtE3hreAK",
        "outputId": "1bd97bc9-2502-4e99-e30b-73c9f11c4171",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----------------------------------------------\n",
            "Bias 0, derivatives from backprop:\n",
            "[[    0.   ]\n",
            " [ -650.398]\n",
            " [ 6087.484]\n",
            " [-1549.527]\n",
            " [   -0.   ]\n",
            " [ 2725.385]]\n",
            "Bias 0, derivatives from finite differences\n",
            "[[    0.   ]\n",
            " [ -650.398]\n",
            " [ 6087.485]\n",
            " [-1549.527]\n",
            " [    0.   ]\n",
            " [ 2725.386]]\n",
            "Success!  Derivatives match.\n",
            "-----------------------------------------------\n",
            "Bias 1, derivatives from backprop:\n",
            "[[3667.49 ]\n",
            " [1070.949]\n",
            " [   0.   ]\n",
            " [  -0.   ]\n",
            " [ 677.495]\n",
            " [   0.   ]]\n",
            "Bias 1, derivatives from finite differences\n",
            "[[3667.49 ]\n",
            " [1070.949]\n",
            " [   0.   ]\n",
            " [   0.   ]\n",
            " [ 677.495]\n",
            " [   0.   ]]\n",
            "Success!  Derivatives match.\n",
            "-----------------------------------------------\n",
            "Bias 2, derivatives from backprop:\n",
            "[[ 173.404]\n",
            " [  33.02 ]\n",
            " [1079.331]\n",
            " [  -0.   ]\n",
            " [2225.069]\n",
            " [  -0.   ]]\n",
            "Bias 2, derivatives from finite differences\n",
            "[[ 173.404]\n",
            " [  33.02 ]\n",
            " [1079.331]\n",
            " [   0.   ]\n",
            " [2225.07 ]\n",
            " [   0.   ]]\n",
            "Success!  Derivatives match.\n",
            "-----------------------------------------------\n",
            "Bias 3, derivatives from backprop:\n",
            "[[ 959.615]\n",
            " [-154.923]\n",
            " [   0.   ]\n",
            " [ 243.5  ]\n",
            " [ 308.033]\n",
            " [ 587.553]]\n",
            "Bias 3, derivatives from finite differences\n",
            "[[ 959.615]\n",
            " [-154.923]\n",
            " [   0.   ]\n",
            " [ 243.5  ]\n",
            " [ 308.033]\n",
            " [ 587.553]]\n",
            "Success!  Derivatives match.\n",
            "-----------------------------------------------\n",
            "Bias 4, derivatives from backprop:\n",
            "[[245.8 ]\n",
            " [ 51.6 ]\n",
            " [  0.  ]\n",
            " [  0.  ]\n",
            " [ -0.  ]\n",
            " [449.67]]\n",
            "Bias 4, derivatives from finite differences\n",
            "[[245.8 ]\n",
            " [ 51.6 ]\n",
            " [  0.  ]\n",
            " [  0.  ]\n",
            " [  0.  ]\n",
            " [449.67]]\n",
            "Success!  Derivatives match.\n",
            "-----------------------------------------------\n",
            "Bias 5, derivatives from backprop:\n",
            "[[-224.574]]\n",
            "Bias 5, derivatives from finite differences\n",
            "[[-224.574]]\n",
            "Success!  Derivatives match.\n",
            "-----------------------------------------------\n",
            "Weight 0, derivatives from backprop:\n",
            "[[    0.   ]\n",
            " [ -780.478]\n",
            " [ 7304.981]\n",
            " [-1859.433]\n",
            " [    0.   ]\n",
            " [ 3270.462]]\n",
            "Weight 0, derivatives from finite differences\n",
            "[[    0.   ]\n",
            " [ -780.478]\n",
            " [ 7304.982]\n",
            " [-1859.433]\n",
            " [    0.   ]\n",
            " [ 3270.463]]\n",
            "Success!  Derivatives match.\n",
            "-----------------------------------------------\n",
            "Weight 1, derivatives from backprop:\n",
            "[[   0.    3480.571 8199.122 4596.465    0.    6321.057]\n",
            " [   0.    1016.366 2394.237 1342.22     0.    1845.82 ]\n",
            " [   0.       0.       0.       0.       0.       0.   ]\n",
            " [   0.       0.       0.       0.       0.       0.   ]\n",
            " [   0.     642.966 1514.623  849.105    0.    1167.689]\n",
            " [   0.       0.       0.       0.       0.       0.   ]]\n",
            "Weight 1, derivatives from finite differences\n",
            "[[   0.    3480.571 8199.123 4596.465    0.    6321.058]\n",
            " [   0.    1016.366 2394.237 1342.22     0.    1845.82 ]\n",
            " [   0.       0.       0.       0.       0.       0.   ]\n",
            " [   0.       0.       0.       0.       0.       0.   ]\n",
            " [   0.     642.966 1514.623  849.105    0.    1167.689]\n",
            " [   0.       0.       0.       0.       0.       0.   ]]\n",
            "Success!  Derivatives match.\n",
            "-----------------------------------------------\n",
            "Weight 2, derivatives from backprop:\n",
            "[[  965.42    518.393     0.        0.      950.565     0.   ]\n",
            " [  183.837    98.714     0.        0.      181.008     0.   ]\n",
            " [ 6009.123  3226.668     0.        0.     5916.66      0.   ]\n",
            " [    0.        0.        0.        0.        0.        0.   ]\n",
            " [12387.966  6651.861     0.        0.    12197.351     0.   ]\n",
            " [    0.        0.        0.        0.        0.        0.   ]]\n",
            "Weight 2, derivatives from finite differences\n",
            "[[  965.42    518.393     0.        0.      950.565     0.   ]\n",
            " [  183.837    98.713     0.        0.      181.008     0.   ]\n",
            " [ 6009.123  3226.668     0.        0.     5916.66      0.   ]\n",
            " [    0.        0.        0.        0.        0.        0.   ]\n",
            " [12387.969  6651.862     0.        0.    12197.354     0.   ]\n",
            " [    0.        0.        0.        0.        0.        0.   ]]\n",
            "Success!  Derivatives match.\n",
            "-----------------------------------------------\n",
            "Weight 3, derivatives from backprop:\n",
            "[[ 6770.045 16352.318  7075.464     0.     5667.161     0.   ]\n",
            " [-1092.975 -2639.963 -1142.282     0.     -914.922     0.   ]\n",
            " [    0.        0.        0.        0.        0.        0.   ]\n",
            " [ 1717.884  4149.365  1795.383     0.     1438.03      0.   ]\n",
            " [ 2173.16   5249.035  2271.198     0.     1819.138     0.   ]\n",
            " [ 4145.166 10012.204  4332.168     0.     3469.892     0.   ]]\n",
            "Weight 3, derivatives from finite differences\n",
            "[[ 6770.046 16352.323  7075.465     0.     5667.162     0.   ]\n",
            " [-1092.975 -2639.963 -1142.282     0.     -914.922     0.   ]\n",
            " [    0.        0.        0.        0.        0.        0.   ]\n",
            " [ 1717.884  4149.365  1795.384     0.     1438.03      0.   ]\n",
            " [ 2173.16   5249.035  2271.198     0.     1819.138     0.   ]\n",
            " [ 4145.167 10012.206  4332.168     0.     3469.892     0.   ]]\n",
            "Success!  Derivatives match.\n",
            "-----------------------------------------------\n",
            "Weight 4, derivatives from backprop:\n",
            "[[1119.576 5393.773    0.    5362.292 1455.409 5006.797]\n",
            " [ 235.031 1132.307    0.    1125.698  305.532 1051.07 ]\n",
            " [   0.       0.       0.       0.       0.       0.   ]\n",
            " [   0.       0.       0.       0.       0.       0.   ]\n",
            " [   0.       0.       0.       0.       0.       0.   ]\n",
            " [2048.169 9867.448    0.    9809.856 2662.547 9159.509]]\n",
            "Weight 4, derivatives from finite differences\n",
            "[[1119.576 5393.774    0.    5362.292 1455.409 5006.798]\n",
            " [ 235.031 1132.307    0.    1125.698  305.532 1051.07 ]\n",
            " [   0.       0.       0.       0.       0.       0.   ]\n",
            " [   0.       0.       0.       0.       0.       0.   ]\n",
            " [   0.       0.       0.       0.       0.       0.   ]\n",
            " [2048.17  9867.45     0.    9809.857 2662.547 9159.511]]\n",
            "Success!  Derivatives match.\n",
            "-----------------------------------------------\n",
            "Weight 5, derivatives from backprop:\n",
            "[[-6781.511 -2603.967     0.        0.        0.    -6184.6  ]]\n",
            "Weight 5, derivatives from finite differences\n",
            "[[-6781.51  -2603.967     0.        0.        0.    -6184.599]]\n",
            "Success!  Derivatives match.\n"
          ]
        }
      ]
    }
  ]
}